# NYC Taxi Batch Pipeline (Spark â†’ BigQuery)

## ğŸ“Œ Overview
This project demonstrates a **batch data pipeline** that ingests NYC Yellow Taxi trip records, processes them with **Apache Spark (PySpark)**, and writes aggregated results into **Google BigQuery** for downstream analytics.  

The goal is to showcase an **end-to-end data engineering workflow** with scalable Spark jobs and cloud-native storage.

---

## âš™ï¸ Architecture

Raw CSV (NYC Taxi Data)
â†“
Spark (PySpark job)
â†“
Curated Data (aggregated hourly fares)
â†“
Google BigQuery (analytics-ready)

---

## ğŸ›  Tech Stack
- **Apache Spark 3.4** (installed via Homebrew, PySpark API in Python)
- **Google BigQuery** (data warehouse, analytics-ready target)
- **Python 3.10** (venv)
- **Airflow 2.9** â†’ Orchestration for scheduling pipeline runs

---

## ğŸ“‚ Project Structure
```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # Sample taxi trip data (tiny CSV or empty with .gitkeep)
â”‚   â””â”€â”€ curated/              # Spark outputs (ignored in Git, .gitkeep to keep folder)
â”‚
â”œâ”€â”€ jobs/
â”‚   â””â”€â”€ nyc_batch.py          # PySpark batch job
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ nyc_taxi_dag.py       # Airflow DAG (orchestration)
â”‚
â”œâ”€â”€ .gitignore                # Git ignore rules
â””â”€â”€ README.md                 # Project documentation
---
```

## ğŸš€ Pipeline Steps
1. **Data Ingestion**  
   - Downloaded sample **NYC Yellow Taxi** CSV data (~50k rows).  
   - Data stored locally under `data/raw/`.

2. **Transformation with Spark**  
   - PySpark job reads CSV.  
   - Performs aggregation of total fares **per pickup hour**.  
   - Writes output to `data/curated/hourly_fares/`.  

3. **Load to BigQuery**  
   - Curated parquet files loaded into BigQuery.  
   - Table partitioned by pickup hour for efficient queries.  

4. **Analytics**  
   - Run SQL queries on BigQuery (e.g., highest fares per hour).  
---

## â–¶ï¸ Running Locally
**Run Spark job directly:**
```
	bash spark-submit --master "local[*]" jobs/nyc_batch.py data/raw/yellow_tripdata_sample.csv data curated/hourly_fares
```
---

## â± Orchestration with Airflow (Optional)
	â€¢	Airflow can schedule and monitor the Spark job.
	â€¢	A DAG (nyc_taxi_dag.py) triggers the PySpark job on a daily schedule.
	â€¢	Provides retries, logging, and UI-based monitoring.
	ğŸ‘‰ See /dags/nyc_taxi_dag.py for details.
---

## ğŸ“Š Sample Output (BigQuery)
```
Row	pickup_hour	trips	avg_fare
1	0	13	10.76923076923077
2	1	10	21.401
3	2	5	20.1
4	4	3	35.54
```
---

## ğŸ”§ Challenges & Learnings
	â€¢	GCP Project ID is required to set at environment level.
	â€¢	Installing dependencies jar gcs-connector-hadoopX.jar and spark-bigquery-with-dependenciesX.jar
	â€¢	Isolated dependencies inside .venv (Airflow + PySpark).
